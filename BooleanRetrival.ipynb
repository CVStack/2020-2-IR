{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Artificial intelligence.txt', 1: 'Computer Science.txt', 2: 'Computer Vision.txt', 3: 'Convolutional_neural_network.txt', 4: 'Deep Learning.txt', 5: 'Information_retrieval.txt', 6: 'Natural_language_processing.txt', 7: 'Operation System.txt', 8: 'clustering.txt', 9: 'machine learning.txt'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from natsort import natsorted\n",
    "import string\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from natsort import natsorted\n",
    "import string\n",
    "import os\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding=\"ascii\", errors=\"surrogateescape\") as f:\n",
    "        stuff = f.read()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return stuff\n",
    "\n",
    "def preprocessing(final_string):\n",
    "    # Tokenize.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    token_list = tokenizer.tokenize(final_string)\n",
    "\n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in token_list]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    "\n",
    "    # Change to lowercase.\n",
    "    token_list = [word.lower() for word in token_list]\n",
    "    return token_list\n",
    "\n",
    "\n",
    "# In this example, we create the positional index for only 1 folder.\n",
    "folder_names = [\"data\"]\n",
    "\n",
    "#stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    "\n",
    "# Initialize the vocabulary.\n",
    "pos_index = {}\n",
    "\n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}\n",
    "\n",
    "for folder_name in folder_names:\n",
    "\n",
    "    # Open files.\n",
    "    file_names = natsorted(os.listdir(folder_name))\n",
    "\n",
    "    # For every file.\n",
    "    for file_name in file_names:\n",
    "        \n",
    "#         print(file_name)\n",
    "        # Read file contents.\n",
    "        if os.path.isdir(file_name):\n",
    "            continue\n",
    "        stuff = read_file(folder_name + '/' + file_name)\n",
    "\n",
    "        # This is the list of words in order of the text.\n",
    "        # We need to preserve the order because we require positions.\n",
    "        # 'preprocessing' function does some basic punctuation removal,\n",
    "        # stopword removal etc.\n",
    "        final_token_list = preprocessing(stuff)\n",
    "#         print(final_token_list[0:20])\n",
    "        \n",
    "        token_list_removed_stop_word = [] #Removal stop_word\n",
    "        for term in final_token_list:\n",
    "            if term not in stop_words: #check words if it is stop word\n",
    "                token_list_removed_stop_word.append(term)\n",
    "                \n",
    "#         print(token_list_removed_stop_word[0:20])\n",
    "#         break\n",
    "        # For position and term in the tokens.\n",
    "        for pos, term in enumerate(token_list_removed_stop_word):\n",
    "            # First stem the term.\n",
    "            term = stemmer.stem(term)\n",
    "\n",
    "            # If term already exists in the positional index dictionary.\n",
    "            if term in pos_index:\n",
    "\n",
    "                # Increment total freq by 1.\n",
    "                pos_index[term][0] = pos_index[term][0] + 1\n",
    "\n",
    "                # Check if the term has existed in that DocID before.\n",
    "                if fileno in pos_index[term][1]:\n",
    "                    pos_index[term][1][fileno].append(pos)\n",
    "\n",
    "                else:\n",
    "                    pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "            # If term does not exist in the positional index dictionary\n",
    "            # (first encounter).\n",
    "            else:\n",
    "\n",
    "                # Initialize the list.\n",
    "                pos_index[term] = []\n",
    "                # The total frequency is 1.\n",
    "                pos_index[term].append(1)\n",
    "                # The postings list is initially empty.\n",
    "                pos_index[term].append({})\n",
    "                # Add doc ID to postings list.\n",
    "                pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "                # Map the file no. to the file name.\n",
    "        file_map[fileno] = file_name\n",
    "\n",
    "        # Increment the file no. counter for document ID mapping\n",
    "        fileno += 1\n",
    "\n",
    "print(file_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index\n",
      "Filename, [Positions]\n",
      "[108, 654, 739, 795, 816, 895, 997, 1004, 1146, 1157, 1169, 1174, 1185, 1189, 1191, 1449, 1459, 1565, 1643, 1849, 1933, 2113, 2309, 2637, 2700, 2757, 2851, 3355, 3368, 3371, 3403, 3563, 3565, 4162, 4251, 4381, 4387, 4536, 4936, 5164, 5241, 5553]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample positional index to test the code.\n",
    "sample_pos_idx = pos_index[\"system\"]\n",
    "print(\"Positional Index\")\n",
    "\n",
    "# print(sample_pos_idx)\n",
    "\n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "# print(file_list)\n",
    "print(file_list.get(0))\n",
    "# for fileno, positions in file_list.items():\n",
    "#     print(fileno, positions)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query after normalization: ['deep', 'neural', 'network', 'and', 'learn', 'not', 'science']\n",
      "Separated terms: [['deep', 'neural', 'network'], ['learn'], ['scienc']]\n",
      "Boolean operators: ['and', 'not']\n",
      "\n",
      "Vectors for query terms [[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0], [1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0]]\n",
      "\n",
      "Intersections [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Search words were found in  ['Convolutional_neural_network.txt']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Start of Boolean Queries\n",
    "query = \"deep neural network and learn not science\"\n",
    "\n",
    "# Tokenizing the the query\n",
    "normalized_query = preprocessing(query)\n",
    "print(\"Boolean Query after normalization:\", normalized_query)\n",
    "\n",
    "# 'connecting_words' means operations (and, or, not)\n",
    "connecting_terms = []\n",
    "cnt = 1\n",
    "\n",
    "# 'different_words' means search words\n",
    "different_terms = []\n",
    "\n",
    "# Splitting the search terms and boolean operations (and, not, or)\n",
    "temp_list = []\n",
    "for term in normalized_query:\n",
    "    # First stem the query terms\n",
    "    \n",
    "    term = stemmer.stem(term)\n",
    "\n",
    "    if term != \"and\" and term != \"or\" and term != \"not\":\n",
    "        temp_list.append(term) # when term is word\n",
    "    else: # when term is connective\n",
    "        connecting_terms.append(term)\n",
    "        \n",
    "        different_terms.append(copy.deepcopy(temp_list))\n",
    "        temp_list.clear()\n",
    "\n",
    "different_terms.append(copy.deepcopy(temp_list))\n",
    "temp_list.clear()\n",
    "\n",
    "print(\"Separated terms:\", different_terms)\n",
    "print(\"Boolean operators:\", connecting_terms)\n",
    "print()\n",
    "\n",
    "# get all unique words from inverted index dictionary\n",
    "unique_words_all = set(pos_index.keys())\n",
    "# print(unique_words_all)\n",
    "# number of documents\n",
    "total_files = fileno + 1  # len(files_with_index)\n",
    "\n",
    "# index of documents, if search word occurs 1, else 0.\n",
    "zeroes_and_ones = []\n",
    "\n",
    "# index of documents for all search words\n",
    "zeroes_and_ones_of_all_terms = []\n",
    "\n",
    "# Find ids for each search word\n",
    "for term in (different_terms):\n",
    "    \n",
    "    if len(term) > 1 : #if term is connected word\n",
    "        zeroes_and_ones = [0] * total_files\n",
    "        docID_list = []\n",
    "        if term[0] in unique_words_all: #tropical --> fish\n",
    "#             print('term : ', term[0])\n",
    "            sample_pos_idx = pos_index[term[0]]\n",
    "            for docID, position in sample_pos_idx[1].items(): # docID : 1, position : [1, 5, 10 ....]\n",
    "                #다음 단어의 docid에서 pos + x가 존재하는지 체크\n",
    "#                 print(\"docID : \", docID)\n",
    "                for p in position:\n",
    "#                     print()\n",
    "#                     print(\"{} --> \".format(p))\n",
    "                    isCorrect = False\n",
    "                    for x in range(1, len(term)):\n",
    "                        search_pos_index = pos_index[term[x]][1].get(docID)\n",
    "                        if search_pos_index is None or not (p + x in search_pos_index):\n",
    "                            break;     \n",
    "#                         print('next term : ', term[x])\n",
    "#                         print(\"{} --> \".format(p + x))\n",
    "                        if x == len(term) - 1:\n",
    "                            isCorrect = True\n",
    "                            \n",
    "                    if isCorrect == True:\n",
    "#                         print('append DocID :', docID)\n",
    "                        docID_list.append(docID)\n",
    "                        break\n",
    "                        \n",
    "            for docID in docID_list:\n",
    "                zeroes_and_ones[docID] = 1\n",
    "            zeroes_and_ones_of_all_terms.append(zeroes_and_ones)\n",
    "                    \n",
    "    else:           \n",
    "        term = term[0]\n",
    "#         print(term)\n",
    "        if term in unique_words_all:\n",
    "            # First set 0 to all documents\n",
    "            zeroes_and_ones = [0] * total_files\n",
    "            sample_pos_idx = pos_index[term]\n",
    "\n",
    "            # Set 1 for document index, if search word occurs\n",
    "            for docId in sample_pos_idx[1].keys():\n",
    "                zeroes_and_ones[docId] = 1\n",
    "\n",
    "            zeroes_and_ones_of_all_terms.append(zeroes_and_ones)\n",
    "\n",
    "        else:\n",
    "            print(term, \" not found\")\n",
    "            sys.exit()\n",
    "\n",
    "# print(zeroes_and_ones_of_all_terms)\n",
    "print(\"Vectors for query terms\", zeroes_and_ones_of_all_terms)\n",
    "print()\n",
    "\n",
    "# Checking intersections\n",
    "for term in connecting_terms:\n",
    "\n",
    "    # Results of search word after or before operation\n",
    "    word_list1 = zeroes_and_ones_of_all_terms[0]\n",
    "    word_list2 = zeroes_and_ones_of_all_terms[1]\n",
    "\n",
    "    if term == \"and\": #0 & 0 = 0 1 & 1 = 1 0 & 1 = 0 1 & 0 = 0\n",
    "        bitwise_op = [w1 & w2 for (w1, w2) in zip(word_list1, word_list2)]\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_terms.insert(0, bitwise_op)\n",
    "\n",
    "    elif term == \"or\":\n",
    "        bitwise_op = [w1 | w2 for (w1, w2) in zip(word_list1, word_list2)]\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_terms.insert(0, bitwise_op)\n",
    "\n",
    "    elif term == \"not\":\n",
    "        bitwise_op = [not w1 for w1 in word_list2]\n",
    "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_terms.remove(word_list1)\n",
    "        bitwise_op = [w1 & w2 for (w1, w2) in zip(word_list1, bitwise_op)]\n",
    "        zeroes_and_ones_of_all_terms.insert(0, bitwise_op)\n",
    "\n",
    "intersections = zeroes_and_ones_of_all_terms[0]\n",
    "print(\"Intersections\", intersections)\n",
    "print()\n",
    "\n",
    "files = []\n",
    "cnt = 0\n",
    "for index in intersections:\n",
    "    if index == 1:\n",
    "        files.append(file_names[cnt])\n",
    "    cnt = cnt + 1\n",
    "\n",
    "print(\"Search words were found in \", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
