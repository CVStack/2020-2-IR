{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.[1] They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.[2][3] They have applications in image and video recognition, recommender systems,[4] image classification, medical image analysis, natural language processing,[5] and financial time series.[6]\n",
      "\n",
      "CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\n",
      "\n",
      "Convolutional networks were inspired by biological processes[7][8][9][10] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n",
      "\n",
      "CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n",
      "\n",
      "The name “convolutional neural\n",
      "network” indicates that the network employs a mathematical operation called\n",
      "convolution. Convolution is a specialized kind of linear operation. Convolutional\n",
      "networks are simply neural networks that use convolution in place of general matrix\n",
      "multiplication in at least one of their layers.[11]\n",
      "\n",
      "A convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that convolve with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.\n",
      "\n",
      "Though the layers are colloquially referred to as convolutions, this is only by convention. Mathematically, it is technically a sliding dot product or cross-correlation. This has significance for the indices in the matrix, in that it affects how weight is determined at a specific index point.\n",
      "\n",
      "When programming a CNN, the input is a tensor with shape (number of images) x (image height) x (image width) x (image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map height) x (feature map width) x (feature map channels). A convolutional layer within a neural network should have the following attributes:\n",
      "\n",
      "Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[12] Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.[13] For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. By using regularized weights over fewer parameters, the vanishing gradient and exploding gradient problems seen during backpropagation in traditional neural networks are avoided.[14][15]\n",
      "\n",
      "Convolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer.[16][17] In addition, pooling may compute a max or an average. Max pooling uses the maximum value from each of a cluster of neurons at the prior layer.[18][19] Average pooling uses the average value from each of a cluster of neurons at the prior layer.[20]\n",
      "\n",
      "Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n",
      "\n",
      "In neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from every element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its receptive field. So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.\n",
      "The subarea of the original input image in the receptive field is increasingly growing as getting deeper in the network architecture. This is due to applying over and over again a convolution which takes into account the value of a specific pixel, but also some surrounding pixels.\n",
      "\n",
      "Each neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.\n",
      "\n",
      "The vector of weights and the bias are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces memory footprint because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.[21]\n",
      "\n",
      "CNN design follows vision processing in living organisms.[citation needed]\n",
      "\n",
      "Work by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[22] Neighboring cells have similar and overlapping receptive fields.[citation needed] Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]\n",
      "\n",
      "Their 1968 paper identified two basic visual cell types in the brain:[8]\n",
      "\n",
      "Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[23][22]\n",
      "\n",
      "The \"neocognitron\"[7] was introduced by Kunihiko Fukushima in 1980.[9][19][24]\n",
      "It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n",
      "\n",
      "In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[25] Max-pooling is often used in modern CNNs.[26]\n",
      "\n",
      "Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[7] Today, however, the CNN architecture is usually trained through backpropagation.\n",
      "\n",
      "The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.[27]\n",
      "\n",
      "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance.[28] It did so by utilizing weight sharing in combination with Backpropagation training.[29] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[28]\n",
      "\n",
      "TDNNs are convolutional networks that share weights along the temporal dimension.[30] They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution.[31] Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both shifts in time and in frequency. This inspired translation invariance in image processing with CNNs.[29] The tiling of neuron outputs can cover timed stages.[32]\n",
      "\n",
      "TDNNs now achieve the best performance in far distance speech recognition.[33]\n",
      "\n",
      "In 1990 Yamaguchi et al. introduced the concept of max pooling. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system.[18] In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n",
      "\n",
      "A system to recognize hand-written ZIP Code numbers[34] involved convolutions in which the kernel coefficients had been laboriously hand designed.[35]\n",
      "\n",
      "Yann LeCun et al. (1989)[35] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\n",
      "\n",
      "This approach became a foundation of modern computer vision.\n",
      "\n",
      "LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998,[36] that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n",
      "\n",
      "Similarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988.[2][3] The architecture and training algorithm were modified in 1991[37] and applied for medical image processing[38] and automatic detection of breast cancer in mammograms.[39]\n",
      "\n",
      "A different convolution-based design was proposed in 1988[40] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[41][42]\n",
      "\n",
      "The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[43] by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n",
      "\n",
      "Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n",
      "\n",
      "In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[44][26] In 2005, another paper also emphasised the value of GPGPU for machine learning.[45]\n",
      "\n",
      "The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[46] Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.[47][48][49][50]\n",
      "\n",
      "In 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark.[51] In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results.[16] In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time.[52] Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions.[53][26] In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).[19]\n",
      "\n",
      "Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[54] A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.[55]\n",
      "\n",
      "Compared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.[56]\n",
      "A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).[57]\n",
      "CHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n",
      "\n",
      "In the past, traditional multilayer perceptron (MLP) models have been used for image recognition.[example  needed] However, due to the full connectivity between nodes, they suffered from the curse of dimensionality, and did not scale well with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.\n",
      "\n",
      "For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n",
      "\n",
      "Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n",
      "\n",
      "Convolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n",
      "\n",
      "Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n",
      "\n",
      "\n",
      "A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n",
      "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[58]\n",
      "[nb 1]\n",
      "\n",
      "Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n",
      "\n",
      "When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n",
      "\n",
      "The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n",
      "\n",
      "Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\n",
      "\n",
      "The spatial size of the output volume can be computed as a function of the input volume size \n",
      "\n",
      "\n",
      "\n",
      "W\n",
      "\n",
      "\n",
      "{\\displaystyle W}\n",
      "\n",
      ", the kernel field size of the convolutional layer neurons \n",
      "\n",
      "\n",
      "\n",
      "K\n",
      "\n",
      "\n",
      "{\\displaystyle K}\n",
      "\n",
      ", the stride with which they are applied \n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "\n",
      "\n",
      "{\\displaystyle S}\n",
      "\n",
      ", and the amount of zero padding \n",
      "\n",
      "\n",
      "\n",
      "P\n",
      "\n",
      "\n",
      "{\\displaystyle P}\n",
      "\n",
      " used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W\n",
      "−\n",
      "K\n",
      "+\n",
      "2\n",
      "P\n",
      "\n",
      "S\n",
      "\n",
      "\n",
      "+\n",
      "1.\n",
      "\n",
      "\n",
      "{\\displaystyle {\\frac {W-K+2P}{S}}+1.}\n",
      "\n",
      "\n",
      "\n",
      "If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be \n",
      "\n",
      "\n",
      "\n",
      "P\n",
      "=\n",
      "(\n",
      "K\n",
      "−\n",
      "1\n",
      ")\n",
      "\n",
      "/\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "{\\textstyle P=(K-1)/2}\n",
      "\n",
      " when the stride is \n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "=\n",
      "1\n",
      "\n",
      "\n",
      "{\\displaystyle S=1}\n",
      "\n",
      " ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n",
      "\n",
      "A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\n",
      "\n",
      "Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n",
      "\n",
      "Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n",
      "\n",
      "Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which max pooling is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.\n",
      "\n",
      "Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by a ReLU layer) in a CNN architecture.[58]:460–461 The pooling operation can be used as another form of translation invariance.[58]:458\n",
      "\n",
      "The pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "\n",
      "X\n",
      ",\n",
      "Y\n",
      "\n",
      "\n",
      "(\n",
      "S\n",
      ")\n",
      "=\n",
      "\n",
      "max\n",
      "\n",
      "a\n",
      ",\n",
      "b\n",
      "=\n",
      "0\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "\n",
      "2\n",
      "X\n",
      "+\n",
      "a\n",
      ",\n",
      "2\n",
      "Y\n",
      "+\n",
      "b\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "{\\displaystyle f_{X,Y}(S)=\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}\n",
      "\n",
      "\n",
      "In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n",
      "\n",
      "In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.[60]\n",
      "\n",
      "Due to the aggressive reduction in the size of the representation,[which?] there is a recent trend towards using smaller filters[61] or discarding pooling layers altogether.[62]\n",
      "\n",
      "\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.[63]\n",
      "\n",
      "Pooling is an important component of convolutional neural networks for object detection based on Fast R-CNN[64] architecture.\n",
      "\n",
      "ReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function \n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "(\n",
      "x\n",
      ")\n",
      "=\n",
      "max\n",
      "(\n",
      "0\n",
      ",\n",
      "x\n",
      ")\n",
      "\n",
      "\n",
      "{\\textstyle f(x)=\\max(0,x)}\n",
      "\n",
      ".[54] It effectively removes negative values from an activation map by setting them to zero.[65] It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n",
      "\n",
      "Other functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent \n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "(\n",
      "x\n",
      ")\n",
      "=\n",
      "tanh\n",
      "⁡\n",
      "(\n",
      "x\n",
      ")\n",
      "\n",
      "\n",
      "{\\displaystyle f(x)=\\tanh(x)}\n",
      "\n",
      ", \n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "(\n",
      "x\n",
      ")\n",
      "=\n",
      "\n",
      "|\n",
      "\n",
      "tanh\n",
      "⁡\n",
      "(\n",
      "x\n",
      ")\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle f(x)=|\\tanh(x)|}\n",
      "\n",
      ", and the sigmoid function \n",
      "\n",
      "\n",
      "\n",
      "σ\n",
      "(\n",
      "x\n",
      ")\n",
      "=\n",
      "(\n",
      "1\n",
      "+\n",
      "\n",
      "e\n",
      "\n",
      "−\n",
      "x\n",
      "\n",
      "\n",
      "\n",
      ")\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}\n",
      "\n",
      ". ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[66]\n",
      "\n",
      "Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). [Reference needed ?]\n",
      "\n",
      "The \"loss layer\" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used.\n",
      "\n",
      "Softmax loss is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in \n",
      "\n",
      "\n",
      "\n",
      "[\n",
      "0\n",
      ",\n",
      "1\n",
      "]\n",
      "\n",
      "\n",
      "{\\displaystyle [0,1]}\n",
      "\n",
      ". Euclidean loss is used for regressing to real-valued labels \n",
      "\n",
      "\n",
      "\n",
      "(\n",
      "−\n",
      "∞\n",
      ",\n",
      "∞\n",
      ")\n",
      "\n",
      "\n",
      "{\\displaystyle (-\\infty ,\\infty )}\n",
      "\n",
      ".\n",
      "\n",
      "CNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n",
      "\n",
      "Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n",
      "\n",
      "The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n",
      "\n",
      "Common filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n",
      "\n",
      "The challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.\n",
      "\n",
      "Typical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers.[67] However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.[60]\n",
      "\n",
      "Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n",
      "\n",
      "Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout.[68][69] At each training stage, individual nodes are either \"dropped out\" of the net with probability \n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "−\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle 1-p}\n",
      "\n",
      " or kept with probability \n",
      "\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle p}\n",
      "\n",
      ", so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n",
      "\n",
      "In the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, however, this probability is typically much lower, since information is directly lost when input nodes are ignored or dropped.\n",
      "\n",
      "At testing time after training has finished, we would ideally like to find a sample average of all possible \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle 2^{n}}\n",
      "\n",
      " dropped-out networks; unfortunately this is unfeasible for large values of \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      ". However, we can find an approximation by using the full network with each node's output weighted by a factor of \n",
      "\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle p}\n",
      "\n",
      ", so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle 2^{n}}\n",
      "\n",
      " neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n",
      "\n",
      "By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.\n",
      "\n",
      "DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability \n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "−\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle 1-p}\n",
      "\n",
      ". Each unit thus receives input from a random subset of units in the previous layer.[70]\n",
      "\n",
      "DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n",
      "\n",
      "A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n",
      "\n",
      "In stochastic pooling,[71] the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\n",
      "\n",
      "An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[72] which delivers excellent performance on the MNIST data set.[72] Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n",
      "\n",
      "Since the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.[73]\n",
      "\n",
      "One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n",
      "\n",
      "Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n",
      "\n",
      "A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n",
      "\n",
      "L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n",
      "\n",
      "L1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n",
      "\n",
      "Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "w\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\vec {w}}}\n",
      "\n",
      " of every neuron to satisfy \n",
      "\n",
      "\n",
      "\n",
      "‖\n",
      "\n",
      "\n",
      "\n",
      "w\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‖\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "<\n",
      "c\n",
      "\n",
      "\n",
      "{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}\n",
      "\n",
      ". Typical values of \n",
      "\n",
      "\n",
      "\n",
      "c\n",
      "\n",
      "\n",
      "{\\displaystyle c}\n",
      "\n",
      " are order of 3–4. Some papers report improvements[74] when using this form of regularization.\n",
      "\n",
      "Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[75]\n",
      "\n",
      "Currently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[76]\n",
      "\n",
      "Thus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[77]\n",
      "\n",
      "CNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported.[19] Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[16] Subsequently, a similar CNN called \n",
      "AlexNet[78] won the ImageNet Large Scale Visual Recognition Challenge 2012.\n",
      "\n",
      "When applied to facial recognition, CNNs achieved a large decrease in error rate.[79] Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\".[10] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[32]\n",
      "\n",
      "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[80] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[81] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[82] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]\n",
      "\n",
      "In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[83]\n",
      "\n",
      "Convolutional neural networks have been used to identify human intention to control assistive devices. There are two ways to use CNNs that accept EMG signals as input. One way is to apply spectogram and map it to 2D features, Then[clarification needed] it can be used similar to image recognition. Another way is to use end-to-end CNNs that directly map raw EMG signals to corresponding classes (classification) or hand/joint position (regression).[84]\n",
      "\n",
      "Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[85][86] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[87][88][89] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[90][91] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[92] and Independent Subspace Analysis.[93]\n",
      "\n",
      "CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[94] search query retrieval,[95] sentence modeling,[96] classification,[97] prediction[98] and other traditional NLP tasks.[99]\n",
      "\n",
      "A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[100]\n",
      "\n",
      "CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design.[101] The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[102] AtomNet discovers chemical features, such as aromaticity, sp3 carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[103] and multiple sclerosis.[104]\n",
      "\n",
      "CNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.[105]\n",
      "\n",
      "CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[106][107] It also earned a win against the program Chinook at its \"expert\" level of play.[108]\n",
      "\n",
      "CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[109] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[110]\n",
      "\n",
      "A couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[111]\n",
      "\n",
      "Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[112][6] Dilated convolutions[113] might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[114] Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[115] Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[116] CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[117] or quantile forecasting[118]).\n",
      "\n",
      "As archaeological findings like clay tablets with cuneiform writing are increasingly acquired using 3D scanners first benchmark datasets are becoming available like HeiCuBeDa[119] providing almost 2.000 normalized 2D- and 3D-datasets prepared with the GigaMesh Software Framework.[120] So curvature based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.[121][122]\n",
      "\n",
      "For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.[123]\n",
      "\n",
      "End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[124] With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[125][126]\n",
      "\n",
      "A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs.[citation needed]\n",
      "\n",
      "Preliminary results were presented in 2014, with an accompanying paper in February 2015.[127] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[128]\n",
      "\n",
      "Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[129] have been obtained using CDBNs.[130]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Artificial intelligence (AI), is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[3] Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".[4]\n",
      "\n",
      "As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.[5] A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\"[6] For instance, optical character recognition is frequently excluded from things considered to be AI,[7] having become a routine technology.[8] Modern machine capabilities generally classified as AI include successfully understanding human speech,[9] competing at the highest level in strategic game systems (such as chess and Go),[10] autonomously operating cars, intelligent routing in content delivery networks, and military simulations.[11]\n",
      "\n",
      "Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism,[12][13] followed by disappointment and the loss of funding (known as an \"AI winter\"),[14][15] followed by new approaches, success and renewed funding.[13][16] For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other.[17] These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"),[18] the use of particular tools (\"logic\" or artificial neural networks), or deep philosophical differences.[21][22][23] Sub-fields have also been based on social factors (particular institutions or the work of particular researchers).[17]\n",
      "\n",
      "The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects.[18] General intelligence is among the field's long-term goals.[24] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.\n",
      "\n",
      "The field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\".[25] This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity.[30] Some people also consider AI to be a danger to humanity if it progresses unabated.[31][32] Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.[33]\n",
      "\n",
      "In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.[34][16]\n",
      "\n",
      "Thought-capable artificial beings appeared as storytelling devices in antiquity,[35] and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Rossum's Universal Robots).[36] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[30]\n",
      "\n",
      "The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis.[37] Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed changing the question from whether a machine was intelligent, to \"whether or not it is possible for machinery to show intelligent behaviour\".[38] The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".[39]\n",
      "\n",
      "The field of AI research was born at a workshop at Dartmouth College in 1956,[40] where the term \"Artificial Intelligence\" was coined by John McCarthy to distinguish the field from cybernetics and escape the influence of the cyberneticist Norbert Wiener.[41] Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research.[42] They and their students produced programs that the press described as \"astonishing\":[43] computers were learning checkers strategies (c. 1954)[44] (and by 1959 were reportedly playing better than the average human),[45] solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English.[46] By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[47] and laboratories had been established around the world.[48] AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[12]\n",
      "\n",
      "They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[49] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\",[14] a period when obtaining funding for AI projects was difficult.\n",
      "\n",
      "In the early 1980s, AI research was revived by the commercial success of expert systems,[50] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[13] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.[15]\n",
      "\n",
      "The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) transistor technology, enabled the development of practical artificial neural network (ANN) technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.[51]\n",
      "\n",
      "In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas.[34] The success was due to increasing computational power (see Moore's law and transistor count), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards.[52] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[53]\n",
      "\n",
      "In 2011, a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[54] Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[55] The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from lengthy AI research[56] as do intelligent personal assistants in smartphones.[57] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[10][58] In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie,[59] who at the time continuously held the world No. 1 ranking for two years.[60][61] This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is a relatively complex game, more so than Chess.\n",
      "\n",
      "According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating the improvements of AI since 2012 supported by lower error rates in image processing tasks.[62] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[16] Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.[62] In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\".[63][64] Around 2016, China greatly accelerated its government funding; given its large supply of data and its rapidly increasing research output, some observers believe it may be on track to becoming an \"AI superpower\".[65][66] However, it has been acknowledged that reports regarding artificial intelligence have tended to be exaggerated.[67][68][69]\n",
      "\n",
      "Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[3] A more elaborate definition characterizes AI as \"a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\"[70]\n",
      "\n",
      "A typical AI analyzes its environment and takes actions that maximize its chance of success.[3] An AI's intended utility function (or goal) can be simple (\"1 if the AI wins a game of Go, 0 otherwise\") or complex (\"Perform actions mathematically similar to ones that succeeded in the past\"). Goals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", goals can be implicitly induced by rewarding some types of behavior or punishing others.[a] Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[71] Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[72] Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to successfully accomplish its narrow classification task.[73]\n",
      "\n",
      "AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute.[b] A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following (optimal for first player) recipe for play at tic-tac-toe:[74]\n",
      "\n",
      "Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or \"rules of thumb\", that have worked well in the past), or can themselves write other algorithms. Some of the \"learners\" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, (given infinite data, time, and memory) learn to approximate any function, including which combination of mathematical functions would best describe the world.[citation needed] These learners could therefore derive all possible knowledge, by considering every possible hypothesis and matching them against the data. In practice, it is seldom possible to consider every possibility, because of the phenomenon of \"combinatorial explosion\", where the time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering a broad range of possibilities unlikely to be beneficial.[75][76] For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered.[77]\n",
      "\n",
      "The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.[78][79]\n",
      "\n",
      "Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, according to Occam's razor principle, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better.\n",
      "\n",
      "Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is.[80] Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[81] A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an \"adversarial\" image that the system misclassifies.[c][82][83]\n",
      "\n",
      "Compared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"naïve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\" (A generic AI has difficulty discerning whether the ones alleged to be advocating violence are the councilmen or the demonstrators[86][87][88]). This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.[89][90][91]\n",
      "\n",
      "The cognitive capabilities of current architectures are very limited, using only a simplified version of what intelligence is really capable of. For instance, the human mind has come up with ways to reason beyond measure and logical explanations to different occurrences in life. What would have been otherwise straightforward, an equivalently difficult problem may be challenging to solve computationally as opposed to using the human mind. This gives rise to two classes of models: structuralist and functionalist. The structural models aim to loosely mimic the basic intelligence operations of the mind such as reasoning and logic. The functional model refers to the correlating data to its computed counterpart.[92]\n",
      "\n",
      "The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[18]\n",
      "\n",
      "Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[93] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[94]\n",
      "\n",
      "These algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.[75] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[95]\n",
      "\n",
      "Knowledge representation[96] and knowledge engineering[97] are central to classical AI research. Some \"expert systems\" attempt to gather explicit knowledge possessed by experts in some narrow domain. In addition, some projects attempt to gather the \"commonsense knowledge\" known to the average person into a database containing extensive knowledge about the world. Among the things a comprehensive commonsense knowledge base would contain are: objects, properties, categories and relations between objects;[98] situations, events, states and time;[99] causes and effects;[100] knowledge about knowledge (what we know about what other people know);[101] and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language.[102] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge[103] by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations can be used in content-based indexing and retrieval,[104] scene interpretation,[105] clinical decision support,[106] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[107] and other areas.[108]\n",
      "\n",
      "Among the most difficult problems in knowledge representation are:\n",
      "\n",
      "Intelligent agents must be able to set goals and achieve them.[115] They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or \"value\") of available choices.[116]\n",
      "\n",
      "In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[117] However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions but also evaluate its predictions and adapt based on its assessment.[118]\n",
      "\n",
      "Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[119]\n",
      "\n",
      "Machine learning (ML), a fundamental concept of AI research since the field's inception,[122] is the study of computer algorithms that improve automatically through experience.[123][124]\n",
      "\n",
      "Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first. Supervised learning includes both classification and numerical regression, which requires a human to label the input data first. Classification is used to determine what category something belongs in, and occurs after a program sees a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change.[124] Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\". Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[125] In reinforcement learning[126] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space.\n",
      "\n",
      "Natural language processing[127] (NLP) allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[128] and machine translation.[129] Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. \"Keyword spotting\" strategies for search are popular and scalable but dumb; a search query for \"dog\" might only match documents with the literal word \"dog\" and miss a document with the word \"poodle\". \"Lexical affinity\" strategies use the occurrence of words such as \"accident\" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well as others, and often achieve acceptable accuracy at the page or paragraph level. Beyond semantic NLP, the ultimate goal of \"narrative\" NLP is to embody a full understanding of commonsense reasoning.[130] By 2019, transformer-based deep learning architectures could generate coherent text.[131]\n",
      "\n",
      "Machine perception[132] is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[133] facial recognition, and object recognition.[134] Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its \"object model\" to assess that fifty-meter pedestrians do not exist.[135]\n",
      "\n",
      "AI is heavily used in robotics.[136] Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage.[137] A modern mobile robot, when given a small, static, and visible environment, can easily determine its location and map its environment; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge. Motion planning is the process of breaking down a movement task into \"primitives\" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object.[138][139][140] Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility\".[141][142] This is attributed to the fact that, unlike checkers, physical dexterity has been a direct target of natural selection for millions of years.[143]\n",
      "\n",
      "Moravec's paradox can be extended to many forms of social intelligence.[145][146] Distributed multi-agent coordination of autonomous vehicles remains a difficult problem.[147] Affective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human affects.[148][149][150] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[151]\n",
      "\n",
      "In the long run, social skills and an understanding of human emotion and game theory would be valuable to a social agent. The ability to predict the actions of others by understanding their motives and emotional states would allow an agent to make better decisions. Some computer systems mimic human emotion and expressions to appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.[152] Similarly, some virtual assistants are programmed to speak conversationally or even to banter humorously; this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[153]\n",
      "\n",
      "Historically, projects such as the Cyc knowledge base (1984–) and the massive Japanese Fifth Generation Computer Systems initiative (1982–1992) attempted to cover the breadth of human cognition. These early projects failed to escape the limitations of non-quantitative symbolic logic models and, in retrospect, greatly underestimated the difficulty of cross-domain AI. Nowadays, most current AI researchers work instead on tractable \"narrow AI\" applications (such as medical diagnosis or automobile navigation).[154] Many researchers predict that such \"narrow AI\" work in different individual domains will eventually be incorporated into a machine with artificial general intelligence (AGI), combining most of the narrow skills mentioned in this article and at some point even exceeding human ability in most or all these areas.[24][155] Many advances have general, cross-domain significance. One high-profile example is that DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own, and later developed a variant of the system which succeeds at sequential learning.[156][157][158] Besides transfer learning,[159] hypothetical AGI breakthroughs could include the development of reflective architectures that can engage in decision-theoretic metareasoning, and figuring out how to \"slurp up\" a comprehensive knowledge base from the entire unstructured Web.[9] Some argue that some kind of (currently-undiscovered) conceptually straightforward, but mathematically difficult, \"Master Algorithm\" could lead to AGI.[160] Finally, a few \"emergent\" approaches look to simulating human intelligence extremely closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges.[161][162]\n",
      "\n",
      "Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.\n",
      "\n",
      "No established unifying theory or paradigm guides AI research. Researchers disagree about many issues.[164] A few of the most long-standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?[21]\n",
      "Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of unrelated problems?[22]\n",
      "\n",
      "In the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England.[165] By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\n",
      "\n",
      "When access to digital computers became possible in the mid-1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and as described below, each one developed its own style of research. John Haugeland named these symbolic approaches to AI \"good old fashioned AI\" or \"GOFAI\".[166] During the 1960s, symbolic approaches had achieved great success at simulating high-level \"thinking\" in small demonstration programs. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.[167]\n",
      "Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n",
      "\n",
      "Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[168][169]\n",
      "\n",
      "Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem-solving, regardless of whether people used the same algorithms.[21] His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[170] Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[171]\n",
      "\n",
      "Researchers at MIT (such as Marvin Minsky and Seymour Papert)[172] found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).[22] Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[173]\n",
      "\n",
      "When computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications.[174] This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software.[50] A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules that illustrate AI.[175] The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\n",
      "\n",
      "By the 1980s, progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems.[23] Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\n",
      "\n",
      "This includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive.[176] Their work revived the non-symbolic point of view of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\n",
      "\n",
      "Within developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).[177][178][179][180]\n",
      "\n",
      "Interest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s.[181] Artificial neural networks are an example of soft computing—they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, Grey system theory, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.[182]\n",
      "\n",
      "Much of traditional GOFAI got bogged down on ad hoc patches to symbolic computation that worked on their own toy models but failed to generalize to real-world results. However, around the 1990s, AI researchers adopted sophisticated mathematical tools, such as hidden Markov models (HMM), information theory, and normative Bayesian decision theory to compare or to unify competing architectures. The shared mathematical language permitted a high level of collaboration with more established fields (like mathematics, economics or operations research).[d] Compared with GOFAI, new \"statistical learning\" techniques such as HMM and neural networks were gaining higher levels of accuracy in many practical domains such as data mining, without necessarily acquiring a semantic understanding of the datasets. The increased successes with real-world data led to increasing emphasis on comparing different approaches against shared test data to see which approach performed best in a broader context than that provided by idiosyncratic toy models; AI research was becoming more scientific. Nowadays results of experiments are often rigorously measurable, and are sometimes (with difficulty) reproducible.[52][183] Different statistical learning techniques have different limitations; for example, basic HMM cannot model the infinite possible combinations of natural language.[184] Critics note that the shift from GOFAI to statistical learning is often also a shift away from explainable AI. In AGI research, some scholars caution against over-reliance on statistical learning, and argue that continuing research into GOFAI will still be necessary to attain general intelligence.[185][186]\n",
      "\n",
      "AI is relevant to any intellectual task.[192] Modern artificial intelligence techniques are pervasive[193] and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[194]\n",
      "\n",
      "High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, predicting flight delays,[195] prediction of judicial decisions,[196] targeting online advertisements, [192][197][198] and energy storage[199]\n",
      "\n",
      "With social media sites overtaking TV as a source for news for young people and news organizations increasingly reliant on social media platforms for generating distribution,[200] major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.[201]\n",
      "\n",
      "AI can also produce Deepfakes, a content-altering technology. ZDNet reports, \"It presents something that did not actually occur,\" Though 88% of Americans believe Deepfakes can cause more harm than good, only 47% of them believe they can be targeted. The boom of election year also opens public discourse to threats of videos of falsified politician media.[202]\n",
      "\n",
      "There are three philosophical questions related to AI[citation needed]:\n",
      "\n",
      "Machines with intelligence have the potential to use their intelligence to prevent harm and minimize the risks; they may have the ability to use ethical reasoning to better choose their actions in the world. As such, there is a need for policy making to devise policies for and regulate artificial intelligence and robotics.[214] Research in this area includes machine ethics, artificial moral agents, friendly AI and discussion towards building a human rights framework is also in talks.[215]\n",
      "\n",
      "Joseph Weizenbaum in Computer Power and Human Reason wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy[217] was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.[218]\n",
      "\n",
      "Wendell Wallach introduced the concept of artificial moral agents (AMA) in his book Moral Machines[219] For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\"[220] and \"Can (Ro)bots Really Be Moral\".[221] For Wallach, the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior, unlike the constraints which society may place on the development of AMAs.[222]\n",
      "\n",
      "The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making.[223] The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\"[224] Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\"[223] that stems from the AAAI Fall 2005 Symposium on Machine Ethics.[224]\n",
      "\n",
      "Political scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent.[225] He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n",
      "\n",
      "One proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI' and will be able to control subsequently developed AIs. Some question whether this kind of check could actually remain in place.\n",
      "\n",
      "Leading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI and the enormity and complexity of building sentient volitional intelligence.\"[226]\n",
      "\n",
      "Lethal autonomous weapons are of concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers and drones.[227]\n",
      "\n",
      "If an AI system replicates all key aspects of human intelligence, will that system also be sentient—will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\n",
      "\n",
      "David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[228] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however human subjective experience is difficult to explain.\n",
      "\n",
      "For example, consider what happens when a person is shown a color swatch and identifies it, saying \"it's red\". The easy problem only requires understanding the machinery in the brain that makes it possible for a person to know that the color swatch is red. The hard problem is that people also know something else—they also know what red looks like. (Consider that a person born blind can know that something is red without knowing what red looks like.)[e] Everyone knows subjective experience exists, because they do it every day (e.g., all sighted people know what red looks like). The hard problem is explaining how the brain creates it, why it exists, and how it is different from knowledge and other aspects of the brain.\n",
      "\n",
      "Computationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing.[229] Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\n",
      "\n",
      "The philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[231] Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be.[232]\n",
      "\n",
      "If a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature.[233] Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. [234] The subject is profoundly discussed in the 2010 documentary film Plug & Pray,[235] and many sci fi media such as Star Trek Next Generation, with the character of Commander Data, who fought being disassembled for research, and wanted to \"become human\", and the robotic holograms in Voyager.\n",
      "\n",
      "Are there limits to how intelligent machines—or human-machine hybrids—can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[155]\n",
      "\n",
      "If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[236] The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\".[237] Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[237][155]\n",
      "\n",
      "Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029 and predicts that the singularity will occur in 2045.[237]\n",
      "\n",
      "Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either.[238] This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.\n",
      "\n",
      "Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[239]\n",
      "\n",
      "The long-term economic effects of AI are uncertain. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit, if productivity gains are redistributed.[240] A February 2020 European Union white paper on artificial intelligence advocated for artificial intelligence for economic benefits, including \"improving healthcare (e.g. making diagnosis more  precise,  enabling  better  prevention  of  diseases), increasing  the  efficiency  of  farming, contributing  to climate  change mitigation  and  adaptation, [and] improving  the  efficiency  of production systems through predictive maintenance\", while acknowledging potential risks.[193]\n",
      "\n",
      "The relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects.[241] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[242] Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\".[243][244][245] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[246] Author Martin Ford and others go further and argue that many jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we're in uncharted territory\" with AI.[33]\n",
      "\n",
      "The potential negative effects of AI and automation were a major issue for Andrew Yang's 2020 presidential campaign in the United States.[247] Irakli Beridze, Head of the Centre for Artificial Intelligence and Robotics at UNICRI, United Nations, has expressed that \"I think the dangerous applications for AI, from my point of view, would be criminals or large terrorist organizations using it to disrupt large processes or simply do pure harm. [Terrorists could cause harm] via digital warfare, or it could be a combination of robotics, drones, with AI and other things as well that could be really dangerous. And, of course, other risks come from things like job losses. If we have massive numbers of people losing jobs and don't find a solution, it will be extremely dangerous. Things like lethal autonomous weapons systems should be properly governed — otherwise there's massive potential of misuse.\"[248]\n",
      "\n",
      "Widespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.[249]\n",
      "\n",
      "Some are concerned about algorithmic bias, that AI programs may unintentionally become biased after processing data that exhibits bias.[250]  Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants.[251]\n",
      "\n",
      "Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".[252][253][254]\n",
      "\n",
      "The development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\n",
      "In his book Superintelligence, philosopher Nick Bostrom provides an argument that artificial intelligence will pose a threat to humankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's—one example is an AI told to compute as many digits of pi as possible—it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.  Bostrom also emphasizes the difficulty of fully conveying humanity's values to an advanced AI.  He uses the hypothetical example of giving an AI the goal to make humans smile to illustrate a misguided attempt.  If the AI in that scenario were to become superintelligent, Bostrom argues, it may resort to methods that most humans would find horrifying, such as inserting \"electrodes into the facial muscles of humans to cause constant, beaming grins\" because that would be an efficient way to achieve its goal of making humans smile.[256]  In his book Human Compatible, AI researcher Stuart J. Russell echoes some of Bostrom's concerns while also proposing an approach to developing provably beneficial machines focused on uncertainty and deference to humans,[257]:173 possibly involving inverse reinforcement learning.[257]:191–193\n",
      "\n",
      "Concern over risk from artificial intelligence has led to some high-profile donations and investments. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1 billion to OpenAI, a nonprofit company aimed at championing responsible AI development.[258] The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[259] Other technology industry leaders believe that artificial intelligence is helpful in its current form and will continue to assist humans. Oracle CEO Mark Hurd has stated that AI \"will actually create more jobs, not less jobs\" as humans will be needed to manage AI systems.[260] Facebook CEO Mark Zuckerberg believes AI will \"unlock a huge amount of positive things,\" such as curing disease and increasing the safety of autonomous cars.[261] In January 2015, Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence.[262] I think there is potentially a dangerous outcome there.\"[263][264]\n",
      "\n",
      "For the danger of uncontrolled advanced AI to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching.[265][266] Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.[267]\n",
      "\n",
      "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI);[268][269] it is therefore related to the broader regulation of algorithms. The regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union.[270] Regulation is considered necessary to both encourage AI and manage associated risks.[271][272] Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.[273]\n",
      "\n",
      "Thought-capable artificial beings appeared as storytelling devices since antiquity,[35]\n",
      "and have been a persistent theme in science fiction.\n",
      "\n",
      "A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[274]\n",
      "\n",
      "Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[275] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[276]\n",
      "\n",
      "Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always an unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\n",
      "\n",
      "Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[277]\n",
      "\n",
      "See also: Logic machines in fiction and List of fictional computers\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Computer science is the study of algorithmic processes and computational machines.[1][2] As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computing systems in hardware and software.[3][4] Computer science addresses any computational problems, especially information processes, such as control, communication, perception, learning, and intelligence.[5][6][7]\n",
      "\n",
      "Its fields can be divided into theoretical and practical disciplines. For example, theory of computation study abstract models of computation and general computational problems that can be solved using them, while computer graphics and computational geometry emphasizes more specific applications. Algorithmics have been called the heart of computer science.[8] Programming language theory considers approaches to the description of computational processes, while software engineering involves the use of programming languages and complex systems. Computer architecture and computer engineering deals with construction of computer components and computer-controlled equipment.[5][9] Human–computer interaction considers the challenges in making computers useful, usable, and accessible. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. According to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\"[10][5]\n",
      "\n",
      "The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. \n",
      "\n",
      "Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[13] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[14] Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[15] He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".[16] \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"[16] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[17] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published [18] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[19] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".[20]\n",
      "\n",
      "During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[21] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[22] Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[23] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[5][24] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[25] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\n",
      "\n",
      "Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population.[26][27] It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704[28] and later the IBM 709[29] computers, which were widely used during the exploration period of such devices. \"Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again\".[26] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.[27]\n",
      "\n",
      "The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947.[30][31] In 1953, the University of Manchester built the first transistorized computer, called the Transistor Computer.[32] However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.[33] The metal–oxide–silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.[34][35] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[33] The MOSFET made it possible to build high-density integrated circuit chips,[36][37] leading to what is known as the computer revolution[38] or microcomputer revolution.[39]\n",
      "\n",
      "Time has seen significant improvements in the usability and effectiveness of computing technology.[40] Modern society has seen a significant shift in the demographics which make use of computer technology; usage has shifted from being mostly exclusive to experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of humanitarian aid was needed for efficient use—in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.\n",
      "\n",
      "Although first proposed in 1956,[27] the term \"computer science\" appears in a 1959 article in Communications of the ACM,[41]\n",
      "in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[42] justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[41]\n",
      "His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[43] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[44] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[45] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\n",
      "\n",
      "In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.[46] Three months later in the same journal, comptologist was suggested, followed next year by hypologist.[47] The term computics has also been suggested.[48] In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics of the University of Edinburgh).[49]  \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"[50]\n",
      "\n",
      "A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\"[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.\n",
      "\n",
      "Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[5] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[27]\n",
      "\n",
      "The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term \"Software Engineering\" means, and how computer science is defined.[51] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[52]\n",
      "\n",
      "The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n",
      "\n",
      "A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[53] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[54] Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[55]\n",
      "Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[56]\n",
      "\n",
      "Computer science is no more about computers than astronomy is about telescopes.\n",
      "As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[57][58]\n",
      "CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[59]—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[57]\n",
      "\n",
      "Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n",
      "\n",
      "According to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\"[5] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n",
      "\n",
      "The famous P = NP? problem, one of the Millennium Prize Problems,[60] is an open problem in the theory of computation.\n",
      "\n",
      "Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[61]\n",
      "Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n",
      "[62]\n",
      "\n",
      "Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n",
      "\n",
      "Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\n",
      "\n",
      "Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[63] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n",
      "\n",
      "Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n",
      "\n",
      "Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[64] The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.\n",
      "\n",
      "Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[65] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[66] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[67]\n",
      "\n",
      "This branch of computer science aims to manage networks between computers worldwide\n",
      "\n",
      "Computer security is a branch of computer technology with an objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.\n",
      "\n",
      "A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n",
      "\n",
      "Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n",
      "\n",
      "Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[68] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[citation needed]\n",
      "\n",
      "Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance.\n",
      "\n",
      "The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[69]\n",
      "\n",
      "Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n",
      "\n",
      "Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[75]\n",
      "\n",
      "Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[76][77] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[78]\n",
      "\n",
      "Computer Science, known by its near synonyms, Computing, Computer Studies, Information Technology (IT) and Information and Computing Technology (ICT), has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students.[79] In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.[80]\n",
      "\n",
      "In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[81] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[82]\n",
      "\n",
      "Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[83][84] and several others are following.[85]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other reasons this message may be displayed:\n",
      "\n",
      "Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]\n",
      "\n",
      "Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[4][5][6]\n",
      "\n",
      "Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9]\n",
      "\n",
      "The adjective \"deep\" in deep learning comes from the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, and then that a network with a nonpolynomial activation function with one hidden layer of unbounded width can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the \"structured\" part.\n",
      "\n",
      "Deep learning is a class of machine learning algorithms that[11](pp199–200) uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\n",
      "\n",
      "Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[12]\n",
      "\n",
      "In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1][13]\n",
      "\n",
      "The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[14] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\n",
      "\n",
      "Deep learning architectures can be constructed with a greedy layer-by-layer method.[15] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]\n",
      "\n",
      "For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\n",
      "\n",
      "Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors[16] and deep belief networks.[1][17]\n",
      "\n",
      "Deep neural networks are generally interpreted in terms of the universal approximation theorem[18][19][20][21][22] or probabilistic inference.[11][12][1][2][17][23]\n",
      "\n",
      "The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[18][19][20][21] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[18][citation needed] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[19] Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.[24]\n",
      "\n",
      "The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[22] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.\n",
      "\n",
      "The probabilistic interpretation[23] derives from the field of machine learning. It features inference,[11][12][1][2][17][23] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[23] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[25] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[26]\n",
      "\n",
      "The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[27] A 1971 paper described a deep network with eight layers trained by the group method of data handling.[28] Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[29]\n",
      "\n",
      "The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[30][16] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[31][32]\n",
      "\n",
      "In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[33][34][35][36] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[37]\n",
      "\n",
      "By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[38][39][40] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.\n",
      "\n",
      "In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[41]\n",
      "\n",
      "In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[42] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[43][44]\n",
      "\n",
      "Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.\n",
      "\n",
      "Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[45][46][47] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[48] Key difficulties have been analyzed, including gradient diminishing[43] and weak temporal correlation structure in neural predictive models.[49][50] Additional difficulties were the lack of training data and limited computing power.\n",
      "\n",
      "Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[51] The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[52]\n",
      "\n",
      "The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[52] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[53]\n",
      "\n",
      "Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[54] LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[55] Later it was combined with connectionist temporal classification (CTC)[56] in stacks of LSTM RNNs.[57] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[58]\n",
      "\n",
      "In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[59]\n",
      "[60][61] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[62] The papers referred to learning for deep belief nets.\n",
      "\n",
      "Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[63][64][65] Convolutional neural networks (CNNs) were superseded for ASR by CTC[56] for LSTM.[54][58][66][67][68][69][70] but are more successful in computer vision.\n",
      "\n",
      "The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[71] Industrial applications of deep learning to large-scale speech recognition started around 2010.\n",
      "\n",
      "The 2009 NIPS Workshop on Deep Learning for Speech Recognition[72] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[73] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[63][74] The nature of the recognition errors produced by the two types of systems was characteristically different,[75][72] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[11][76][77] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[75][72] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[63][75][73][78]\n",
      "\n",
      "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[79][80][81][76]\n",
      "\n",
      "Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[82] That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[83] In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.[84][85][86] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[87][88] Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.[89]\n",
      "\n",
      "In 2012, a team led by George E. Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug.[90][91] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.[92][93][94]\n",
      "\n",
      "Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs on GPUs were needed to progress on computer vision.[84][86][37][95][2] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[96] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[97] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.[98]\n",
      "\n",
      "Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[99][100][101][102]\n",
      "\n",
      "Some researchers state that the October 2012 ImageNet victory anchored the start of a \"deep learning revolution\" that has transformed the AI industry.[103]\n",
      "\n",
      "In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\n",
      "\n",
      "Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\n",
      "\n",
      "An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\n",
      "\n",
      "Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n",
      "\n",
      "The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\n",
      "\n",
      "Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n",
      "\n",
      "As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing \"Go\"[104] ).\n",
      "\n",
      "A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[12][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\n",
      "\n",
      "DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[105] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[12]\n",
      "\n",
      "Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\n",
      "\n",
      "DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[106] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\n",
      "\n",
      "Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[107][108][109][110][111] Long short-term memory is particularly effective for this use.[54][112]\n",
      "\n",
      "Convolutional deep neural networks (CNNs) are used in computer vision.[113] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[70]\n",
      "\n",
      "As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\n",
      "\n",
      "DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[28] or weight decay (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ℓ\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle \\ell _{2}}\n",
      "\n",
      "-regularization) or sparsity (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ℓ\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle \\ell _{1}}\n",
      "\n",
      "-regularization) can be applied during training to combat overfitting.[114] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[115] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[116]\n",
      "\n",
      "DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[117] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[118][119]\n",
      "\n",
      "Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[120][121]\n",
      "\n",
      "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[122] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[123] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[124][125]\n",
      "\n",
      "Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[112] is competitive with traditional speech recognizers on certain tasks.[55]\n",
      "\n",
      "The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[126] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n",
      "\n",
      "The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[11][78][76]\n",
      "\n",
      "All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[11][131][132]\n",
      "\n",
      "Electromyography (EMG) signals have been used extensively in the identification of user intention to potentially control assistive devices such as smart wheelchairs, exoskeletons, and prosthetic devices. In the past century feed forward dense neural network has been used. Then, researcher used spectrogram to map EMG signal  and then use it as input of deep convolutional neural networks. Recently, end-to-end deep learning is used to map raw signals directly to identification of user intention [133].\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[134]\n",
      "\n",
      "Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011.[135]\n",
      "\n",
      "Deep learning-trained vehicles now interpret 360° camera views.[136] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n",
      "\n",
      "Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[137][138]\n",
      "\n",
      "Neural networks have been used for implementing language models since the early 2000s.[107] LSTM helped to improve machine translation and language modeling.[108][109][110]\n",
      "\n",
      "Other key techniques in this field are negative sampling[139] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[140] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[140] Deep neural architectures provide the best results for constituency parsing,[141] sentiment analysis,[142] information retrieval,[143][144] spoken language understanding,[145] machine translation,[108][146] contextual entity linking,[146] writing style recognition,[147] Text classification and others.[148]\n",
      "\n",
      "Recent developments generalize word embedding to sentence embedding.\n",
      "\n",
      "Google Translate (GT) uses a large end-to-end long short-term memory network.[149][150][151][152][153][154] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples.\"[150] It translates \"whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[150] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[150][155] GT uses English as an intermediate between most language pairs.[155]\n",
      "\n",
      "A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[156][157] Research has explored use of deep learning to predict the biomolecular targets,[90][91] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[92][93][94]\n",
      "\n",
      "AtomNet is a deep learning system for structure-based rational drug design.[158] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[159] and multiple sclerosis.[160][161]\n",
      "\n",
      "In 2019 generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[162][163]\n",
      "\n",
      "Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[164]\n",
      "\n",
      "Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[165][166] Multi-view deep learning has been applied for learning user preferences from multiple domains.[167] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n",
      "\n",
      "An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[168]\n",
      "\n",
      "In medical informatics, deep learning was used to predict sleep quality based on data from wearables[169] and predictions of health complications from electronic health record data.[170]\n",
      "\n",
      "Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[171][172]\n",
      "\n",
      "Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[173] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n",
      "\n",
      "Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[174] These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[175] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n",
      "\n",
      "Deep learning is being successfully applied to financial fraud detection and anti-money laundering. \"Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events\". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.\n",
      "[176]\n",
      "\n",
      "The United States Department of Defense applied deep learning to train robots in new tasks through observation.[177]\n",
      "\n",
      "Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[178][179][180][181] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.\"[182]\n",
      "\n",
      "A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[183][184] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[185][186] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[187]\n",
      "\n",
      "Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[188][189] and neural populations.[190] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[191] both at the single-unit[192] and at the population[193] levels.\n",
      "\n",
      "Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[194]\n",
      "\n",
      "Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[195][196][197] Google Translate uses a neural network to translate between more than 100 languages.\n",
      "\n",
      "In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.[198]\n",
      "\n",
      "In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[199]\n",
      "\n",
      "As of 2008,[200] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[177] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[177] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[201]\n",
      "\n",
      "Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n",
      "\n",
      "A main criticism concerns the lack of theory surrounding some methods.[202] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[203]\n",
      "\n",
      "\n",
      "Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:\n",
      "\"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\"[204]\n",
      "In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[205] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[206] website.\n",
      "\n",
      "Some deep learning architectures display problematic behaviors,[207] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images[208] and misclassifying minuscule perturbations of correctly classified images.[209] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[207] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[210] decompositions of observed entities and events.[207] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[211] and artificial intelligence (AI).[212]\n",
      "\n",
      "As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception.[213] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.”[214] In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[215] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[216]\n",
      "\n",
      "Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[215]\n",
      "\n",
      "ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[215]\n",
      "\n",
      "Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.[215]\n",
      "\n",
      "In “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[215]\n",
      "\n",
      "Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[217] The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[217] Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture.[218] This user interface is a mechanism to generate \"a constant stream of  verification data\"[217] to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\".[217]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.[1][2][3]\n",
      "\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]\n",
      "\n",
      "The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\n",
      "\n",
      "Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.[6]\n",
      "\n",
      "Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3] \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[9] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[10] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n",
      "\n",
      "In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it \"describe what it saw\".[12][13]\n",
      "\n",
      "What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]\n",
      "\n",
      "The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[14]\n",
      "By the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]\n",
      "\n",
      "Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.[15][16] \n",
      "The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.[citation needed]\n",
      "\n",
      "Areas of artificial intelligence deal with autonomous path planning or deliberation for robotic systems to navigate through an environment.[17] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.\n",
      "\n",
      "Artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.\n",
      "\n",
      "Computer vision is often considered to be part of information engineering.[18][19]\n",
      "\n",
      "Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.\n",
      "\n",
      "A third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how \"real\" vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in biology.\n",
      "\n",
      "Some strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[20]\n",
      "\n",
      "Yet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision that have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n",
      "\n",
      "Beside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion ecommerce, inventory management, patent search, furniture, and the beauty industry.[citation needed]\n",
      "\n",
      "The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.\n",
      "\n",
      "Computer graphics produces image data from 3D models, computer vision often produces 3D models from image data.[21] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.\n",
      "\n",
      "The following characterizations appear relevant but should not be taken as universally accepted::\n",
      "\n",
      "Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n",
      "\n",
      "Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n",
      "\n",
      "One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is detection of tumours, arteriosclerosis or other malign changes; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images interpreted by humans—ultrasonic images or X-ray images for example—to reduce the influence of noise.\n",
      "\n",
      "A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.[25]\n",
      "\n",
      "Military applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\n",
      "\n",
      "One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.\n",
      "\n",
      "Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting micro undulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins is being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data of the imperfections on a very large surface.[26] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[27]\n",
      "\n",
      "Other application areas include:\n",
      "\n",
      "Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n",
      "\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]\n",
      "\n",
      "The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature:[citation needed]\n",
      "\n",
      "Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans.[29] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease[citation needed].\n",
      "\n",
      "Several specialized tasks based on recognition exist, such as:\n",
      "\n",
      "Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images. Examples of such tasks are:\n",
      "\n",
      "Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[21]\n",
      "\n",
      "The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look, to distinguish them from noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n",
      "\n",
      "An example in this field is inpainting.\n",
      "\n",
      "The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n",
      "\n",
      "Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\n",
      "\n",
      "The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n",
      "\n",
      "While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[34]\n",
      "\n",
      "There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories such as camera supports, cables and connectors.\n",
      "\n",
      "Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n",
      "\n",
      "A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n",
      "\n",
      "While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.[35]\n",
      "\n",
      "Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\n",
      "\n",
      "As of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and graphics processing units (GPUs) in this role.[36]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\n",
      "\n",
      "Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\n",
      "\n",
      "An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.\n",
      "\n",
      "An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.[1]\n",
      "\n",
      "Depending on the application the data objects may be, for example, text documents, images,[2] audio,[3] mind maps[4] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\n",
      "\n",
      "Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.[5]\n",
      "\n",
      "there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute\n",
      "The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945.[6] It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film.[7] The first description of a computer searching for information was described by Holmstrom in 1948,[8] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents).[6] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n",
      "\n",
      "In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n",
      "\n",
      "For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n",
      "\n",
      "The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval[clarification needed] or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\n",
      "\n",
      "Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n",
      "\n",
      "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
      "\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others.\n",
      "\n",
      "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[12][13] such as by writing grammars or devising heuristic rules for stemming.\n",
      "\n",
      "More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: \n",
      "\n",
      "Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used\n",
      "\n",
      "Since the so-called \"statistical revolution\"[14][15] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\n",
      "\n",
      "Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\n",
      "\n",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since the early 2010s,[16] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).\n",
      "\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
      "\n",
      "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[25] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[26] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[27] George Lakoff offers a methodology to build Natural language processing (NLP) algorithms through the perspective of Cognitive science, along with the findings of Cognitive linguistics:[28]\n",
      "\n",
      "The first defining aspect of this cognitive task of NLP is the application of the theory of Conceptual metaphor, explained by Lakoff as “the understanding of one idea, in terms of another” which provides an idea of the intent of the author.[29]\n",
      "\n",
      "For example, consider some of the meanings, in English, of the word “big”. When used as a Comparative, as in “That is a big tree,” a likely inference of the intent of the author is that the author is using the word “big” to imply a statement about the tree being ”physically large” in comparison to other trees or the authors experience.  When used as a Stative verb, as in ”Tomorrow is a big day”, a likely inference of the author’s intent it that ”big” is being used to imply ”importance”.  These examples are not presented to be complete, but merely as indicators of the implication of the idea of Conceptual metaphor.  The intent behind other usages, like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\n",
      "\n",
      "This leads to the second defining aspect of this cognitive task of NLP, namely Probabilistic context-free grammar (PCFG) which enables cognitive NLP algorithms to assign relative measures of meaning  to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed. The mathematical equation for such algorithms is presented in US patent 9269353 :\n",
      "\n",
      "Where,\n",
      "     RMM, is the Relative Measure of Meaning\n",
      "     token, is any block of text, sentence, phrase or word\n",
      "     N, is the number of tokens being analyzed\n",
      "     PMM, is the Probable Measure of Meaning based on a corpora\n",
      "     d, is the location of the token along the sequence of N-1 tokens\n",
      "     PF, is the Probability Function specific to a language\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1][2] It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[3] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
      "\n",
      "Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[5][6] In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "Simple Definition: Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
      "\n",
      "Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.[7][8]\n",
      "\n",
      "The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.[7][8]\n",
      "\n",
      "\n",
      "Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n",
      "\n",
      "Other approaches have been developed which don't fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example topic modeling, dimensionality reduction or meta learning.[9]\n",
      "\n",
      "As of 2020, deep learning has become the dominant approach for much ongoing work in the field of machine learning.[7]\n",
      "\n",
      "The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence.[10][11] A representative book of the machine learning research during the 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[12] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[13] In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[14]\n",
      "\n",
      "Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[16]\n",
      "\n",
      "As a scientific endeavor, machine learning grew out of the quest for artificial intelligence. In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what was then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[17] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[18]:488\n",
      "\n",
      "However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[18]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[19] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[18]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[18]:25\n",
      "\n",
      "Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[19] As of 2019, many sources continue to assert that machine learning remains a subfield of AI. Yet some practitioners, for example, Dr Daniel Hulme, who teaches AI and runs a company operating in the field, argues that machine learning and AI are separate.[8][20][7]\n",
      "\n",
      "Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n",
      "\n",
      "Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]\n",
      "\n",
      "Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[22] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[23] He also suggested the term data science as a placeholder to call the overall field.[23]\n",
      "\n",
      "Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[24] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest.\n",
      "\n",
      "Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[25]\n",
      "\n",
      "A core objective of a learner is to generalize from its experience.[4][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n",
      "\n",
      "The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\n",
      "\n",
      "For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]\n",
      "\n",
      "In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n",
      "\n",
      "The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.\n",
      "\n",
      "Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[28] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[29] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[15]\n",
      "\n",
      "Types of supervised learning algorithms include active learning, classification and regression.[30] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. \n",
      "\n",
      "Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
      "\n",
      "Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function.[31] Though unsupervised learning encompasses other domains involving summarizing and explaining data features.\n",
      "\n",
      "Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n",
      "\n",
      "Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels,  yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\n",
      "\n",
      "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[32]\n",
      "\n",
      "Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[33] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n",
      "\n",
      "Self-learning as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).[34] It is a learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[35]\n",
      "The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n",
      "\n",
      "It is a system with only one input, situation s, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[36]\n",
      "\n",
      "Several learning algorithms aim at discovering better representations of the inputs provided during training.[37] Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n",
      "\n",
      "Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[38] and various forms of clustering.[39][40][41]\n",
      "\n",
      "Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[42] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[43]\n",
      "\n",
      "Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations thorough examination, without relying on explicit algorithms.\n",
      "\n",
      "Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[44] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[45]\n",
      "\n",
      "In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[46] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[47]\n",
      "\n",
      "In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[48]\n",
      "\n",
      "Three broad categories of anomaly detection techniques exist.[49] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n",
      "\n",
      "In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies and imitation.\n",
      "\n",
      "Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[50]\n",
      "\n",
      "Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[51] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n",
      "\n",
      "Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[52] For example, the rule \n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\n",
      "o\n",
      "n\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ",\n",
      "p\n",
      "o\n",
      "t\n",
      "a\n",
      "t\n",
      "o\n",
      "e\n",
      "s\n",
      "\n",
      "}\n",
      "⇒\n",
      "{\n",
      "\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n",
      "\n",
      " found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n",
      "\n",
      "Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[53]\n",
      "\n",
      "Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n",
      "\n",
      "Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[54][55][56] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[57] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n",
      "\n",
      "Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.\n",
      "\n",
      "Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n",
      "\n",
      "An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n",
      "\n",
      "The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n",
      "\n",
      "Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[58]\n",
      "\n",
      "Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.\n",
      "\n",
      "Support vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.[59]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n",
      "\n",
      "Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization (mathematics) methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[60]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. \n",
      "\n",
      "A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n",
      "\n",
      "A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[61][62] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[63]\n",
      "\n",
      "Usually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model.\n",
      "\n",
      "Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[64]\n",
      "\n",
      "There are many applications for machine learning, including:\n",
      "\n",
      "In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[66] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[67] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[68] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[69] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences among artists.[70] In 2019 Springer Nature published the first research book created using machine learning.[71]\n",
      "\n",
      "Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[72][73][74] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[75]\n",
      "\n",
      "In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[76] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[77][78]\n",
      "\n",
      "Machine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society.[79] Language models learned from data have been shown to contain human-like biases.[80][81] Machine learning systems used for criminal risk assessment have been found to be biased against black people.[82][83] In 2015, Google photos would often tag black people as gorillas,[84] and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all.[85] Similar issues with recognizing non-white people have been found in many other systems.[86] In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[87] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[88] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.”[89]\n",
      "\n",
      "Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[90]\n",
      "\n",
      "In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).[91]\n",
      "\n",
      "Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[92] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[93][94] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\n",
      "\n",
      "Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[95][96]\n",
      "\n",
      "Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these \"greed\" biases are addressed.[97]\n",
      "\n",
      "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units.[98] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[99] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[100][101]\n",
      "\n",
      "Software suites containing a variety of machine learning algorithms include the following:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering can refer to the following:\n",
      "\n",
      "In computing:\n",
      "\n",
      "In economics:\n",
      "\n",
      "In graph theory:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#WebScrapping\n",
    "\n",
    "\n",
    "\n",
    "search_list = ['Convolutional_neural_network', 'Artificial intelligence', 'Computer Science', 'Operation System', 'Deep Learning', 'Computer Vision', \n",
    "              'Information_retrieval', 'Natural_language_processing', 'machine learning', 'clustering']\n",
    "\n",
    "index = 1\n",
    "for search_term in search_list:\n",
    "    page = requests.get('https://en.wikipedia.org/wiki/' + search_term) #get resuource \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    soup = soup.find('div', {'id' : 'mw-content-text'})\n",
    "\n",
    "    data = soup.find_all('p')\n",
    "    \n",
    "    file_name = 'data/' + search_term +'.txt'\n",
    "    f = open(file_name, 'w')\n",
    "    \n",
    "    for text in data:\n",
    "        f.write(text.get_text())\n",
    "        print(text.get_text())\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from natsort import natsorted\n",
    "import string\n",
    "import os\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding=\"ascii\", errors=\"surrogateescape\") as f:\n",
    "        stuff = f.read()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return stuff\n",
    "\n",
    "def preprocessing(final_string):\n",
    "    # Tokenize.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    token_list = tokenizer.tokenize(final_string)\n",
    "\n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in token_list]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    "\n",
    "    # Change to lowercase.\n",
    "    token_list = [word.lower() for word in token_list]\n",
    "    return token_list\n",
    "\n",
    "\n",
    "# In this example, we create the positional index for only 1 folder.\n",
    "folder_names = [\"data\"]\n",
    "\n",
    "#stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    "\n",
    "# Initialize the vocabulary.\n",
    "pos_index = {}\n",
    "\n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}\n",
    "\n",
    "for folder_name in folder_names:\n",
    "\n",
    "    # Open files.\n",
    "    file_names = natsorted(os.listdir(folder_name))\n",
    "\n",
    "    # For every file.\n",
    "    for file_name in file_names:\n",
    "\n",
    "        # Read file contents.\n",
    "        if os.path.isdir(file_name):\n",
    "            continue\n",
    "        stuff = read_file(folder_name + '/' + file_name)\n",
    "\n",
    "        # This is the list of words in order of the text.\n",
    "        # We need to preserve the order because we require positions.\n",
    "        # 'preprocessing' function does some basic punctuation removal,\n",
    "        # stopword removal etc.\n",
    "        final_token_list = preprocessing(stuff)\n",
    "#         print(final_token_list[0:20])\n",
    "        \n",
    "        token_list_removed_stop_word = [] #Removal stop_word\n",
    "        for term in final_token_list:\n",
    "            if term not in stop_words: #check words if it is stop word\n",
    "                token_list_removed_stop_word.append(term)\n",
    "                \n",
    "#         print(token_list_removed_stop_word[0:20])\n",
    "#         break\n",
    "        # For position and term in the tokens.\n",
    "        for pos, term in enumerate(token_list_removed_stop_word):\n",
    "            # First stem the term.\n",
    "            term = stemmer.stem(term)\n",
    "\n",
    "            # If term already exists in the positional index dictionary.\n",
    "            if term in pos_index:\n",
    "\n",
    "                # Increment total freq by 1.\n",
    "                pos_index[term][0] = pos_index[term][0] + 1\n",
    "\n",
    "                # Check if the term has existed in that DocID before.\n",
    "                if fileno in pos_index[term][1]:\n",
    "                    pos_index[term][1][fileno].append(pos)\n",
    "\n",
    "                else:\n",
    "                    pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "            # If term does not exist in the positional index dictionary\n",
    "            # (first encounter).\n",
    "            else:\n",
    "\n",
    "                # Initialize the list.\n",
    "                pos_index[term] = []\n",
    "                # The total frequency is 1.\n",
    "                pos_index[term].append(1)\n",
    "                # The postings list is initially empty.\n",
    "                pos_index[term].append({})\n",
    "                # Add doc ID to postings list.\n",
    "                pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "                # Map the file no. to the file name.\n",
    "        file_map[fileno] = file_name\n",
    "\n",
    "        # Increment the file no. counter for document ID mapping\n",
    "        fileno += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index\n",
      "[399, {0: [47, 173, 209, 530, 841, 845, 1086, 1089, 1135, 1236, 1239, 1265, 1431, 1453, 1478, 1588, 1622, 2161, 2180, 2193, 2239, 2248, 2259, 2274, 2410, 2501, 2843, 2854, 2860, 3251, 3414, 3497, 3517, 3641, 3701, 3721, 3734, 5470], 1: [40, 127, 1771], 2: [86, 145, 430, 475, 484, 493, 553, 715, 1067, 1591, 1903], 3: [1, 166, 362, 660, 921, 1121, 1129, 1335, 1391, 1400, 1720, 1801, 2144, 2161, 2568, 2651, 2976, 3189, 3585, 3626, 3980, 4065, 4085, 4183, 4188, 4246, 4382, 4407, 4570, 4579, 4583, 4588, 4612], 4: [1, 6, 11, 18, 19, 27, 118, 143, 163, 182, 185, 222, 254, 256, 302, 304, 329, 337, 391, 393, 397, 434, 438, 446, 456, 459, 479, 483, 635, 688, 715, 731, 734, 842, 858, 871, 1025, 1145, 1192, 1213, 1215, 1304, 1309, 1364, 1385, 1396, 1490, 1526, 1557, 1587, 1601, 1649, 1679, 1710, 1834, 1868, 1909, 1952, 1967, 2041, 2483, 2555, 2585, 2631, 2659, 2662, 2664, 2818, 2862, 2957, 3034, 3160, 3222, 3242, 3279, 3305, 3320, 3322, 3352, 3370, 3413, 3434, 3439, 3452, 3471, 3490, 3500, 3506, 3518, 3526, 3553, 3561, 3578, 3641, 3663, 3709, 3724, 3755, 3768, 3797, 3809, 3821, 3837, 3867, 3871, 3880, 3919, 3936, 3961, 3976, 3988, 4005, 4042, 4107, 4154, 4167, 4196, 4420, 4429, 4529, 4602], 6: [122, 159, 164, 230, 257, 266, 337, 453, 466, 520], 9: [1, 16, 36, 55, 75, 86, 94, 102, 111, 118, 126, 128, 139, 162, 190, 240, 252, 264, 272, 276, 285, 289, 307, 312, 317, 340, 364, 369, 389, 420, 433, 476, 500, 558, 597, 615, 622, 631, 637, 658, 664, 671, 696, 739, 745, 792, 800, 816, 827, 864, 873, 879, 891, 900, 929, 938, 945, 1005, 1011, 1013, 1032, 1041, 1046, 1060, 1110, 1112, 1139, 1145, 1149, 1186, 1190, 1196, 1218, 1235, 1245, 1261, 1273, 1331, 1334, 1340, 1366, 1370, 1388, 1391, 1427, 1437, 1445, 1458, 1463, 1470, 1485, 1520, 1571, 1581, 1600, 1605, 1642, 1649, 1655, 1657, 1671, 1674, 1676, 1683, 1698, 1702, 1710, 1720, 1723, 1737, 1757, 1768, 1772, 1807, 1809, 1833, 1838, 1867, 2049, 2053, 2072, 2079, 2082, 2101, 2105, 2108, 2119, 2133, 2148, 2151, 2156, 2260, 2270, 2277, 2285, 2290, 2292, 2294, 2380, 2424, 2442, 2461, 2536, 2625, 2647, 2655, 2681, 2754, 2938, 2989, 2999, 3010, 3021, 3050, 3053, 3061, 3089, 3109, 3187, 3211, 3219, 3247, 3251, 3301, 3317, 3325, 3344, 3356, 3367, 3426, 3439, 3450, 3454, 3507, 3644, 3678, 3703, 3714, 3769, 3796, 3811, 3844, 3868]}]\n",
      "Filename, [Positions]\n",
      "Artificial intelligence.txt [47, 173, 209, 530, 841, 845, 1086, 1089, 1135, 1236, 1239, 1265, 1431, 1453, 1478, 1588, 1622, 2161, 2180, 2193, 2239, 2248, 2259, 2274, 2410, 2501, 2843, 2854, 2860, 3251, 3414, 3497, 3517, 3641, 3701, 3721, 3734, 5470]\n",
      "Computer Science.txt [40, 127, 1771]\n",
      "Computer Vision.txt [86, 145, 430, 475, 484, 493, 553, 715, 1067, 1591, 1903]\n",
      "Convolutional_neural_network.txt [1, 166, 362, 660, 921, 1121, 1129, 1335, 1391, 1400, 1720, 1801, 2144, 2161, 2568, 2651, 2976, 3189, 3585, 3626, 3980, 4065, 4085, 4183, 4188, 4246, 4382, 4407, 4570, 4579, 4583, 4588, 4612]\n",
      "Deep Learning.txt [1, 6, 11, 18, 19, 27, 118, 143, 163, 182, 185, 222, 254, 256, 302, 304, 329, 337, 391, 393, 397, 434, 438, 446, 456, 459, 479, 483, 635, 688, 715, 731, 734, 842, 858, 871, 1025, 1145, 1192, 1213, 1215, 1304, 1309, 1364, 1385, 1396, 1490, 1526, 1557, 1587, 1601, 1649, 1679, 1710, 1834, 1868, 1909, 1952, 1967, 2041, 2483, 2555, 2585, 2631, 2659, 2662, 2664, 2818, 2862, 2957, 3034, 3160, 3222, 3242, 3279, 3305, 3320, 3322, 3352, 3370, 3413, 3434, 3439, 3452, 3471, 3490, 3500, 3506, 3518, 3526, 3553, 3561, 3578, 3641, 3663, 3709, 3724, 3755, 3768, 3797, 3809, 3821, 3837, 3867, 3871, 3880, 3919, 3936, 3961, 3976, 3988, 4005, 4042, 4107, 4154, 4167, 4196, 4420, 4429, 4529, 4602]\n",
      "Natural_language_processing.txt [122, 159, 164, 230, 257, 266, 337, 453, 466, 520]\n",
      "machine learning.txt [1, 16, 36, 55, 75, 86, 94, 102, 111, 118, 126, 128, 139, 162, 190, 240, 252, 264, 272, 276, 285, 289, 307, 312, 317, 340, 364, 369, 389, 420, 433, 476, 500, 558, 597, 615, 622, 631, 637, 658, 664, 671, 696, 739, 745, 792, 800, 816, 827, 864, 873, 879, 891, 900, 929, 938, 945, 1005, 1011, 1013, 1032, 1041, 1046, 1060, 1110, 1112, 1139, 1145, 1149, 1186, 1190, 1196, 1218, 1235, 1245, 1261, 1273, 1331, 1334, 1340, 1366, 1370, 1388, 1391, 1427, 1437, 1445, 1458, 1463, 1470, 1485, 1520, 1571, 1581, 1600, 1605, 1642, 1649, 1655, 1657, 1671, 1674, 1676, 1683, 1698, 1702, 1710, 1720, 1723, 1737, 1757, 1768, 1772, 1807, 1809, 1833, 1838, 1867, 2049, 2053, 2072, 2079, 2082, 2101, 2105, 2108, 2119, 2133, 2148, 2151, 2156, 2260, 2270, 2277, 2285, 2290, 2292, 2294, 2380, 2424, 2442, 2461, 2536, 2625, 2647, 2655, 2681, 2754, 2938, 2989, 2999, 3010, 3021, 3050, 3053, 3061, 3089, 3109, 3187, 3211, 3219, 3247, 3251, 3301, 3317, 3325, 3344, 3356, 3367, 3426, 3439, 3450, 3454, 3507, 3644, 3678, 3703, 3714, 3769, 3796, 3811, 3844, 3868]\n",
      "\n",
      "Positional Index\n",
      "[198, {0: [108, 654, 739, 795, 816, 895, 997, 1004, 1146, 1157, 1169, 1174, 1185, 1189, 1191, 1449, 1459, 1565, 1643, 1849, 1933, 2113, 2309, 2637, 2700, 2757, 2851, 3355, 3368, 3371, 3403, 3563, 3565, 4162, 4251, 4381, 4387, 4536, 4936, 5164, 5241, 5553], 1: [24, 90, 211, 276, 1070, 1089, 1096, 1349, 1371, 1435, 1449, 1574, 1661, 1722, 1750, 1871, 1905, 1939, 1943, 1975, 2015, 2097], 2: [21, 97, 129, 177, 212, 242, 256, 521, 536, 587, 619, 656, 679, 693, 699, 777, 1006, 1076, 1219, 1303, 1341, 1344, 1346, 1354, 1863, 1866, 1896, 1917, 1919, 2021, 2047, 2054, 2056, 2063, 2070, 2080, 2098, 2144, 2166, 2178, 2202], 3: [40, 1024, 1071, 1073, 1098, 3596, 3609, 3687, 4073, 4542], 4: [93, 338, 791, 1312, 1471, 1480, 1497, 1502, 1636, 1822, 1842, 1941, 1943, 1951, 2794, 3159, 3243, 3302, 3483, 3554, 3730, 3753, 3767, 4035, 4176, 4212, 4291, 4327, 4398, 4430, 4533, 4603], 5: [6, 38, 45, 47, 69, 161, 164, 169, 281, 322, 325, 379, 419, 424], 6: [106, 199, 219, 334, 342, 385, 408, 491], 9: [108, 226, 253, 265, 478, 490, 1209, 1420, 1503, 1522, 2130, 2153, 2159, 2185, 2272, 2338, 2392, 2443, 2449, 2451, 2460, 3305, 3326, 3368, 3419, 3649, 3679, 3698, 3732]}]\n",
      "Filename, [Positions]\n",
      "Artificial intelligence.txt [108, 654, 739, 795, 816, 895, 997, 1004, 1146, 1157, 1169, 1174, 1185, 1189, 1191, 1449, 1459, 1565, 1643, 1849, 1933, 2113, 2309, 2637, 2700, 2757, 2851, 3355, 3368, 3371, 3403, 3563, 3565, 4162, 4251, 4381, 4387, 4536, 4936, 5164, 5241, 5553]\n",
      "Computer Science.txt [24, 90, 211, 276, 1070, 1089, 1096, 1349, 1371, 1435, 1449, 1574, 1661, 1722, 1750, 1871, 1905, 1939, 1943, 1975, 2015, 2097]\n",
      "Computer Vision.txt [21, 97, 129, 177, 212, 242, 256, 521, 536, 587, 619, 656, 679, 693, 699, 777, 1006, 1076, 1219, 1303, 1341, 1344, 1346, 1354, 1863, 1866, 1896, 1917, 1919, 2021, 2047, 2054, 2056, 2063, 2070, 2080, 2098, 2144, 2166, 2178, 2202]\n",
      "Convolutional_neural_network.txt [40, 1024, 1071, 1073, 1098, 3596, 3609, 3687, 4073, 4542]\n",
      "Deep Learning.txt [93, 338, 791, 1312, 1471, 1480, 1497, 1502, 1636, 1822, 1842, 1941, 1943, 1951, 2794, 3159, 3243, 3302, 3483, 3554, 3730, 3753, 3767, 4035, 4176, 4212, 4291, 4327, 4398, 4430, 4533, 4603]\n",
      "Information_retrieval.txt [6, 38, 45, 47, 69, 161, 164, 169, 281, 322, 325, 379, 419, 424]\n",
      "Natural_language_processing.txt [106, 199, 219, 334, 342, 385, 408, 491]\n",
      "machine learning.txt [108, 226, 253, 265, 478, 490, 1209, 1420, 1503, 1522, 2130, 2153, 2159, 2185, 2272, 2338, 2392, 2443, 2449, 2451, 2460, 3305, 3326, 3368, 3419, 3649, 3679, 3698, 3732]\n",
      "\n",
      "Positional Index\n",
      "[226, {0: [119, 181, 246, 726, 980, 1254, 1426, 3044, 3137, 3528, 3539, 3645], 1: [1432, 1593, 1934, 1957, 2300], 2: [1626, 1648, 1705], 3: [4, 10, 25, 66, 75, 117, 165, 186, 191, 203, 206, 219, 330, 360, 415, 450, 455, 541, 552, 620, 635, 662, 943, 957, 967, 995, 1093, 1158, 1190, 1200, 1254, 1266, 1314, 1362, 1368, 1384, 1396, 1521, 1613, 1635, 1673, 1726, 1731, 1800, 1855, 1863, 2027, 2220, 2236, 2405, 2449, 2517, 2536, 2555, 2590, 2838, 2848, 2855, 2898, 2911, 2945, 2969, 3137, 3143, 3180, 3207, 3210, 3223, 3307, 3479, 3487, 3584, 3746, 3753, 3811, 3832, 3857, 3952, 4067, 4084, 4181, 4276, 4291, 4330, 4334, 4348, 4353, 4365, 4380, 4398, 4464, 4487, 4500, 4506, 4517, 4521, 4575, 4619, 4625, 4630, 4644], 4: [16, 31, 34, 37, 40, 56, 84, 101, 123, 132, 227, 231, 248, 359, 362, 374, 419, 504, 509, 537, 590, 593, 607, 614, 626, 670, 705, 743, 776, 876, 883, 917, 922, 930, 937, 962, 1022, 1105, 1120, 1135, 1201, 1283, 1345, 1607, 1697, 1932, 1938, 1947, 2076, 2100, 2104, 2116, 2126, 2143, 2159, 2163, 2185, 2211, 2230, 2263, 2290, 2319, 2344, 2369, 2540, 2553, 2595, 2845, 2858, 2999, 3057, 3142, 3177, 3267, 3575, 3583, 3670, 3680, 3699, 3784, 4078, 4205, 4584], 6: [422, 451], 9: [339, 444, 527, 1476, 1666, 1928, 2446, 2456, 2604, 2616, 2632, 2750, 2895, 2897, 2917, 2926, 2940, 2952, 2955, 3806]}]\n",
      "Filename, [Positions]\n",
      "Artificial intelligence.txt [119, 181, 246, 726, 980, 1254, 1426, 3044, 3137, 3528, 3539, 3645]\n",
      "Computer Science.txt [1432, 1593, 1934, 1957, 2300]\n",
      "Computer Vision.txt [1626, 1648, 1705]\n",
      "Convolutional_neural_network.txt [4, 10, 25, 66, 75, 117, 165, 186, 191, 203, 206, 219, 330, 360, 415, 450, 455, 541, 552, 620, 635, 662, 943, 957, 967, 995, 1093, 1158, 1190, 1200, 1254, 1266, 1314, 1362, 1368, 1384, 1396, 1521, 1613, 1635, 1673, 1726, 1731, 1800, 1855, 1863, 2027, 2220, 2236, 2405, 2449, 2517, 2536, 2555, 2590, 2838, 2848, 2855, 2898, 2911, 2945, 2969, 3137, 3143, 3180, 3207, 3210, 3223, 3307, 3479, 3487, 3584, 3746, 3753, 3811, 3832, 3857, 3952, 4067, 4084, 4181, 4276, 4291, 4330, 4334, 4348, 4353, 4365, 4380, 4398, 4464, 4487, 4500, 4506, 4517, 4521, 4575, 4619, 4625, 4630, 4644]\n",
      "Deep Learning.txt [16, 31, 34, 37, 40, 56, 84, 101, 123, 132, 227, 231, 248, 359, 362, 374, 419, 504, 509, 537, 590, 593, 607, 614, 626, 670, 705, 743, 776, 876, 883, 917, 922, 930, 937, 962, 1022, 1105, 1120, 1135, 1201, 1283, 1345, 1607, 1697, 1932, 1938, 1947, 2076, 2100, 2104, 2116, 2126, 2143, 2159, 2163, 2185, 2211, 2230, 2263, 2290, 2319, 2344, 2369, 2540, 2553, 2595, 2845, 2858, 2999, 3057, 3142, 3177, 3267, 3575, 3583, 3670, 3680, 3699, 3784, 4078, 4205, 4584]\n",
      "Natural_language_processing.txt [422, 451]\n",
      "machine learning.txt [339, 444, 527, 1476, 1666, 1928, 2446, 2456, 2604, 2616, 2632, 2750, 2895, 2897, 2917, 2926, 2940, 2952, 2955, 3806]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample positional index to test the code.\n",
    "sample_pos_idx = pos_index[\"learn\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)\n",
    "\n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)\n",
    "print()\n",
    "\n",
    "sample_pos_idx = pos_index[\"system\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)\n",
    "\n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)\n",
    "print()\n",
    "\n",
    "sample_pos_idx = pos_index[\"network\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)\n",
    "\n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
